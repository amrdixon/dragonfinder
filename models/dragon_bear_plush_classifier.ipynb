{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dragon vs. Bear Classifier\n",
    "\n",
    "The goal of this work is to build a model that classifies images as either a dragon or a bear. Alas, dragons are fickle creatures and rarely seen in the wild. So, this preliminary classifier is build to recognize plush dragons and bears. This notebook shows the first proof of concept of the dragon vs. bear classifier. The image dataset is composed of some possibly copyrighted data and thus not included for distribution in the repo. As such, this notebook is intended to show the model development process but is not reproducible as written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1338)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration\n",
    "\n",
    "The dataset is 634 images or plush bears and dragons scraped from the web. The dataset is pretty well balanced with 334 images of dragons and 300 images of bears. The images are loaded as TensorFlow datasets. Loading into TensorFlow datasets accomplishes a few things including warping all images to the same size, shuffling the dataset, separating them into training and validation sets and defining batch size. Because our training set is small, batch size in this work is defined as 1. This means the model is trained with stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 634 files belonging to 2 classes.\n",
      "Using 508 files for training.\n",
      "Found 634 files belonging to 2 classes.\n",
      "Using 126 files for validation.\n"
     ]
    }
   ],
   "source": [
    "def load_preprocess_data(data_dir='../data', batch_size=1, img_height=160, img_width=160, display_sample=False):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses training and validation data.\n",
    "    \n",
    "    args:\n",
    "    data_dir (str, default='./data'): directory containing the images,\n",
    "    each class should have its own directory with the class name being the directory name\n",
    "    batch_size (int, default=1): batch size, default is set to 1 and training performs stochastic grad descemnt\n",
    "    img_height (int, default=160): image height after preprocessing\n",
    "    img_width (int, default=160): image width after preprocessing\n",
    "    display (bool, default=True): display samples from data\n",
    "    \n",
    "    returns:\n",
    "    train_ds, val_test_ds (tf.data.Dataset, tf.data.Dataset): training and validation_test datasets\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        #Use keras to load train ds and val ds\n",
    "        train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            data_dir,\n",
    "            shuffle=True,\n",
    "            labels='inferred',\n",
    "            validation_split=0.2,\n",
    "            subset=\"training\",\n",
    "            seed=123,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        val_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            data_dir,\n",
    "            shuffle=True,\n",
    "            labels='inferred',\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\",\n",
    "            seed=123,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=batch_size)\n",
    "        \n",
    "        #Display samples from a batch\n",
    "        if display_sample:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            for images, labels in train_ds.take(1):\n",
    "                for i in range(min(9, len(labels))):\n",
    "                    ax = plt.subplot(3, 3, i + 1)\n",
    "                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "                    plt.title(train_ds.class_names[labels[i]])\n",
    "                    plt.axis(\"off\")\n",
    "                    \n",
    "        return train_ds, val_test_ds\n",
    "        \n",
    "    except:\n",
    "        print(\"Could not load train and validation datasets.\")\n",
    "        \n",
    "train_ds, val_test_ds = load_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The classifier uses Google's pre-trained MobileNetV2 convolutional neural network for feature extraction. This architecture is lightweight which lends to flexibility for deployment in web or embedded environments. The pre-trained MobileNetV2 network was trained in the ImageNet dataset (1.4 million images comprising 1000 classes). We use transfer learning to take advantage of the of all the features already learned by the pre-trained architecture.\n",
    "\n",
    "The complete flow of the new model is as follows:\n",
    "1. Preprocess images: Rescales the pixel values from [0,255] to [-1,1].\n",
    "2. Perform data augmentation (optional): Randomly performs horizontal flips and up to 10% rotation to emulate a more realistic image capture.\n",
    "3. Load MobileNetV2 neural network: This model is pre-trained with the ImageNet dataset and the last layer is not included.\n",
    "4. Freeze model: All weights of the model so far won't be updated during training.\n",
    "5. Add new classification layers: Add a global average pooling layer and a fully connected layer to output a single value representing the two classes. The output is a logit.\n",
    "\n",
    "This model uses the Adam optimizer to optimize a cross entropy (log loss) cost function. The learning rate is small (0.0001), because the stochastic gradient descent (rather than mini-batch) is more stable for small learning rates. The training data set is small and thus training is not very time constrained. Therefore, I allowed the model to train for 20 epochs to ensure it reached an optimal value. \n",
    "\n",
    "The learning curve shows the model's accuracy on the training and validation sets after each epoch. They converge nicely as training progresses. Final accuracy on the validation set is 97.62%. Variance is low (i.e. the model is not overfitting) since the training and validation accuracy are comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "508/508 [==============================] - 20s 34ms/step - loss: 0.6292 - accuracy: 0.7137 - val_loss: 0.3798 - val_accuracy: 0.7698\n",
      "Epoch 2/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.3722 - accuracy: 0.8449 - val_loss: 0.2890 - val_accuracy: 0.8571\n",
      "Epoch 3/20\n",
      "508/508 [==============================] - 14s 29ms/step - loss: 0.2603 - accuracy: 0.8991 - val_loss: 0.2380 - val_accuracy: 0.8730\n",
      "Epoch 4/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.2225 - accuracy: 0.9187 - val_loss: 0.2144 - val_accuracy: 0.9048\n",
      "Epoch 5/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.1720 - accuracy: 0.9426 - val_loss: 0.1883 - val_accuracy: 0.9048\n",
      "Epoch 6/20\n",
      "508/508 [==============================] - 14s 27ms/step - loss: 0.1681 - accuracy: 0.9507 - val_loss: 0.1668 - val_accuracy: 0.9127\n",
      "Epoch 7/20\n",
      "508/508 [==============================] - 14s 27ms/step - loss: 0.1586 - accuracy: 0.9469 - val_loss: 0.1538 - val_accuracy: 0.9206\n",
      "Epoch 8/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.1378 - accuracy: 0.9483 - val_loss: 0.1416 - val_accuracy: 0.9444\n",
      "Epoch 9/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.1446 - accuracy: 0.9525 - val_loss: 0.1295 - val_accuracy: 0.9603\n",
      "Epoch 10/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.1141 - accuracy: 0.9600 - val_loss: 0.1281 - val_accuracy: 0.9603\n",
      "Epoch 11/20\n",
      "508/508 [==============================] - 15s 29ms/step - loss: 0.0996 - accuracy: 0.9691 - val_loss: 0.1128 - val_accuracy: 0.9603\n",
      "Epoch 12/20\n",
      "508/508 [==============================] - 15s 29ms/step - loss: 0.0967 - accuracy: 0.9696 - val_loss: 0.1119 - val_accuracy: 0.9603\n",
      "Epoch 13/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.0756 - accuracy: 0.9727 - val_loss: 0.1059 - val_accuracy: 0.9683\n",
      "Epoch 14/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.1029 - accuracy: 0.9602 - val_loss: 0.1030 - val_accuracy: 0.9683\n",
      "Epoch 15/20\n",
      "508/508 [==============================] - 14s 28ms/step - loss: 0.0749 - accuracy: 0.9817 - val_loss: 0.1030 - val_accuracy: 0.9683\n",
      "Epoch 16/20\n",
      "483/508 [===========================>..] - ETA: 0s - loss: 0.0694 - accuracy: 0.9785"
     ]
    }
   ],
   "source": [
    "def build_model(IMG_SHAPE, augment_data=True, base_learning_rate = 0.0001):\n",
    "    \"\"\"\n",
    "    Builds a binary image classifer tensorflow model. This model uses the MobileNet V2\n",
    "    convolutional nn for feature extraction. Transfer learning is performed to use the model for this\n",
    "    use case. Leveraging transfer learning of the pretrained MobileNet V2 neural network allows us to build\n",
    "    a deep learning classifier with a lot less data.\n",
    "    \n",
    "    args:\n",
    "    IMG_SHAPE ((int,int,int)): shape of the input images, (height,width,channels)\n",
    "    augment_data (bool, default=True): If true, data augmentation is performed\n",
    "    base_learning_rate (float, default=0.0001): learning rate for grad descent during training\n",
    "    \n",
    "    returns:\n",
    "    model (tf.keras.Model)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    \n",
    "    if augment_data:\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        ])\n",
    "    \n",
    "    #use the MobileNet V2 architecture for the base conv net\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    #freeze these layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    #Build the classification head\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    prediction_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    #build model using Keras functional API\n",
    "    inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    if augment_data:\n",
    "        x = data_augmentation(inputs)\n",
    "        x = preprocess_input(x)\n",
    "    else:\n",
    "        x = preprocess_input(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    #compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#compile model\n",
    "model = build_model(train_ds.element_spec[0].shape[-3:])\n",
    "\n",
    "#train model\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=20,\n",
    "                    validation_data=val_test_ds)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(history):\n",
    "    \"\"\"\n",
    "    Plots the learning curves for model training.\n",
    "    \n",
    "    args:\n",
    "    history (tf.keras.callbacks.History): history callbrack returned from model training\n",
    "    \n",
    "    returns:\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curve(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "We look at a couple different metrics for our final analysis on the validation dataset.\n",
    "\n",
    "First and foremost, we examine the overall accuracy since this was the classifier's optimizing metric. The test set here is pretty small (126 images), which diminishes the significance of the final performance metrics. However, the classifier's overall accuracy was 96.8% which I consider to be adequate.\n",
    "\n",
    "A confusion matrix shows the breakdown of actual labels vs. classification. The true positive (bear) rate was 98.1%, while the true negative (dragon) rate was lower at 95.9%. In other words, the classifier more accurately classifies an image of a bear given that the image is a bear than it does a dragon given that the image is a dragon. To give a balanced view of how well the classifier classifiers dragons and bears, I calculated the balanced accuracy  at 97.0%.\n",
    "\n",
    "For extra visual validation, I show a sample of correctly and incorrectly labeled images below. Given the small validation set and even smaller number of incorrectly classified images (i.e. 4 images), it is not immediately obvious at this time why those images were not correctly classified by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache some data for performance analysis\n",
    "val_test_ds_cahced = val_test_ds.cache()\n",
    "\n",
    "#show a confusion matrix\n",
    "def plot_conf_mat(tf_model, ds, class_names):\n",
    "    \n",
    "    #run inference on dataset\n",
    "    predictions = tf_model.predict(ds).squeeze()\n",
    "    #convert logits to classes\n",
    "    logit_to_pred = lambda x: x>=0\n",
    "    #convert to int\n",
    "    predictions = logit_to_pred(predictions).astype(int)\n",
    "    \n",
    "    #create a labels array\n",
    "    labels = ds.map(lambda img, label: int(label == 1))\n",
    "    labels = np.fromiter(labels.as_numpy_iterator(), int)\n",
    "    \n",
    "    num_predict0_label0 = sum((predictions==0) & (labels==0))\n",
    "    num_predict1_label1 = sum((predictions==1) & (labels==1))\n",
    "    num_predict0_label1 = sum((predictions==0) & (labels==1))\n",
    "    num_predict1_label0 = sum((predictions==1) & (labels==0))\n",
    "    \n",
    "    conf_matrix_vals = [[num_predict0_label0, num_predict0_label1],[num_predict1_label0, num_predict1_label1]]\n",
    "\n",
    "    ax = sns.heatmap(conf_matrix_vals, cmap=\"YlGnBu\", annot=True, xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_xlabel('Labels', fontsize='large')\n",
    "    ax.set_ylabel('Predictions', fontsize='large')\n",
    "    \n",
    "    return conf_matrix_vals\n",
    "\n",
    "conf_matrix_vals = plot_conf_mat(model, val_test_ds_cahced, val_test_ds.class_names)\n",
    "\n",
    "\n",
    "#Look at some metrics\n",
    "print('Overall acccuracy: ', (conf_matrix_vals[0][0] + conf_matrix_vals[1][1])/(conf_matrix_vals[0][0] + conf_matrix_vals[1][1]+conf_matrix_vals[0][1] + conf_matrix_vals[1][0]))\n",
    "print('True positive (bear) rate: ', (conf_matrix_vals[0][0])/(conf_matrix_vals[0][0] + conf_matrix_vals[1][0]))\n",
    "print('True negative (dragon) rate: ', (conf_matrix_vals[1][1])/(conf_matrix_vals[1][1] + conf_matrix_vals[0][1]))\n",
    "print('Balanced Accuracy: ', (conf_matrix_vals[0][0])/(conf_matrix_vals[0][0] + conf_matrix_vals[1][0])/2 + (conf_matrix_vals[1][1])/(conf_matrix_vals[1][1] + conf_matrix_vals[0][1])/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_predicted_images(tf_model, ds, class_names, disp_mislabeled=True):\n",
    "    \"\"\"\n",
    "    Displays images and their predctions, either all incorrect or all correct predictions.\n",
    "    \n",
    "    args:\n",
    "    tf_model (tf.keras.Model): compiled and trained tf model\n",
    "    ds (tf.data.Dataset): cached dataset to perform predictions on\n",
    "    class_names (list): list of class names\n",
    "    disp_mislabeled (bool, default=True): If true, displays incorrect predictions, else correct predictions.\n",
    "    \n",
    "    returns:\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #run inference on dataset\n",
    "    predictions = tf_model.predict(ds).squeeze()\n",
    "    #convert logits to classes\n",
    "    logit_to_pred = lambda x: x>=0\n",
    "    #convert to int\n",
    "    predictions = logit_to_pred(predictions).astype(int)\n",
    "    \n",
    "    #create a labels array\n",
    "    labels = ds.map(lambda img, label: int(label == 1))\n",
    "    labels = np.fromiter(labels.as_numpy_iterator(), int)\n",
    "\n",
    "    \n",
    "    #compare labels and predictions to find correct/incorrect classifications\n",
    "    if disp_mislabeled:\n",
    "        labels_mask = predictions != labels\n",
    "    else:\n",
    "        labels_mask = predictions == labels\n",
    "    labels_indices = np.where(labels_mask)[0]\n",
    "\n",
    "    #if number of indices >9, plot first 9\n",
    "    if len(labels_indices) > 9:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        indices_rand = labels_indices[np.random.randint(0,len(labels_indices), 9)]\n",
    "        for i, ds_i in enumerate(indices_rand):\n",
    "            image,label = list(ds.as_numpy_iterator())[ds_i]\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(image.squeeze().astype(\"uint8\"))\n",
    "            plt.title(class_names[predictions[ds_i]])\n",
    "            plt.axis(\"off\")\n",
    "    #else plot all\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i, ds_i in enumerate(labels_indices):\n",
    "            image,label = list(ds.as_numpy_iterator())[ds_i]\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(image.squeeze().astype(\"uint8\"))\n",
    "            plt.title(class_names[predictions[ds_i]])\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "disp_predicted_images(model, val_test_ds_cahced, val_test_ds.class_names, False)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_predicted_images(model, val_test_ds_cahced, val_test_ds.class_names, True)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "After model training and tuning was completed, I exported the model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "export_path = os.path.join(os.getcwd(), 'dragon_bear_classifier_mobilenetv2', str(version))\n",
    "print('export_path = {}\\n'.format(export_path))\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    export_path,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dragonfinder",
   "language": "python",
   "name": "dragonfinder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
